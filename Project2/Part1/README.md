To start the project, you have to,
1. We need to start the 3-node spark cluster based on HDFS
2. Before storing the data in HDFS, we nee to download the training and testing dataset from Kaggle to the cluster by running the following two commands: 
    wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7" --header="Accept-Language: en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7" --header="Referer: https://www.kaggle.com/" "https://storage.googleapis.com/kagglesdsdata/competitions/8076/44219/train.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1683297491&Signature=YKcHFrAPRnN4535OHOO%2FXTz4gq1BfONtl5QT3vClp%2BhbUN0IsZP6dEUmYct8W0Sz%2FPmW5NnNIJJyggcsSsdy6KXGOGJZCNnDjvpW%2B%2F06Egmc9ccoT0bCWzR7%2BLkPS1sbXL4zqpC2XQK3seMi9EE1udWjY921fU8I1leKK22H2Cwsnj%2BUoyVcyxH5ckq09cveOgXHitDgWwxIThsjvzibEBRhePE3THQe1G7eQZF0BbH9glxg5qWOWlXYimO0bOCpKzuGNykjaPqqUFKPGjwRM43GC9Fc9CTC4VE1gf04FsFoYYtcFW0JG9XJguB8yH3UAdQO4FrhsRRWo28AR4CyCg%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain.csv.zip" -c -O 'train.csv.zip'

    wget --header="Host: storage.googleapis.com" --header="User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7" --header="Accept-Language: en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7" --header="Referer: https://www.kaggle.com/" "https://storage.googleapis.com/kagglesdsdata/competitions/8076/44219/test.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1683297536&Signature=H%2BeZqRxpDznfC%2BU%2BJclLlZT2ZenHOHk0FZ8PwtP0RZYYqtQJSv8M4QhQ0McWQsbjdrGRVn1Mrsguf3sw2hkZULdNxvbrZu0p3hD2N42zwFm%2FaIjmUbIgZW54MG3IC8SmmVJ1zcvfvN1IDeRsIBYK5iitLwpopqhZHfsYWPvGsU3dybkyYKvCXSSyZQRXIa%2F%2BA5QLoIoP0Wl2fJTPjUPdJ8ZTDG0gz66cZCYCHi0lNfVcM1AJ%2BgpBVLiLTe1xcmSVujhX4E3BfxsbzhNBXPJ6o6pm%2FoLYcj8wRDqOatrjp66lrlxRRA5d4lAdJCC6veaVOOnSEUI9fWnUtlwG1HDjVg%3D%3D&response-content-disposition=attachment%3B+filename%3Dtest.csv.zip" -c -O 'test.csv.zip'
3. Since the datasets are stored in a zip file, we need to unzip file so our algorithm can read the dataset
   1. First, install unzip to unzip the file by running "sudo apt install unzip"
   2. Second, run "unzip train.csv.zip" and "unzip test.csv.zip"
   3. Make sure the datasets are at the correct path in the cluster, or you can change the path in test.sh to read the files
4. Run test.sh.
